# -*- coding: utf-8 -*-
"""Heart_Attack_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ELmXVdgvfCrgsWEo7dlGFsktyQMeoU-x

# HEART ATTCK PREDICTION

December 8, **2024**

Heart Attack Prediction using Machine Learning involves analyzing various health-related factors and medical history to determine whether a person is at risk of a heart attack. In this project, multiple machine learning models—such as Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, Random Forest, Support Vector Machine (SVM), Naive Bayes, and Gradient Boosting—are applied to find the most accurate and reliable model for prediction. The goal is to build an effective system that can assist in early diagnosis and preventive care by identifying high-risk individuals using data-driven methods.
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import io

zip_file = 'archive (1).zip'

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall('extracted_data')

import pandas as pd

df = pd.read_csv('extracted_data/heart_attack_prediction_dataset.csv')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df

df.shape

df.dtypes

df.describe()

df.info()

df.corr(numeric_only=True)

""" **DATA CLEANING**"""

df.isna().sum()

df.duplicated().sum()

df['Sex'].value_counts()

df['Physical Activity Days Per Week'].value_counts()

df['Blood Pressure'].value_counts()

"""OUTLIER DETECTION"""

import matplotlib.pyplot as plt
plt.figure(figsize=(5,4))
plt.boxplot(df['Age'])
plt.title('Age')
plt.show()

df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)

q1_systolic = df['Systolic'].quantile(0.25)
q1_diastolic = df['Diastolic'].quantile(0.25)

print("Q1 Systolic:", q1_systolic)
print("Q1 Diastolic:", q1_diastolic)

df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)

q1_sys = df['Systolic'].quantile(0.25)
q3_sys = df['Systolic'].quantile(0.75)

q1_dia = df['Diastolic'].quantile(0.25)
q3_dia = df['Diastolic'].quantile(0.75)

iqr_sys = q3_sys - q1_sys
iqr_dia = q3_dia - q1_dia

min_sys = q1_sys - 1.5 * iqr_sys
max_sys = q3_sys + 1.5 * iqr_sys

min_dia = q1_dia - 1.5 * iqr_dia
max_dia = q3_dia + 1.5 * iqr_dia

print("Systolic BP IQR:", iqr_sys)
print("Systolic Min Range:", min_sys)
print("Systolic Max Range:", max_sys)

print("Diastolic BP IQR:", iqr_dia)
print("Diastolic Min Range:", min_dia)
print("Diastolic Max Range:", max_dia)

q1_sys = df['Systolic'].quantile(0.25)
q3_sys = df['Systolic'].quantile(0.75)
iqr_sys = q3_sys - q1_sys
min_sys = q1_sys - 1.5 * iqr_sys
max_sys = q3_sys + 1.5 * iqr_sys

q1_dia = df['Diastolic'].quantile(0.25)
q3_dia = df['Diastolic'].quantile(0.75)
iqr_dia = q3_dia - q1_dia
min_dia = q1_dia - 1.5 * iqr_dia
max_dia = q3_dia + 1.5 * iqr_dia

df = df[
    (df['Systolic'] >= min_sys) & (df['Systolic'] <= max_sys) &
    (df['Diastolic'] >= min_dia) & (df['Diastolic'] <= max_dia)
]

print("Filtered DataFrame shape:", df.shape)

import matplotlib.pyplot as plt

plt.figure(figsize=(5, 4))
plt.boxplot(df['Systolic'])
plt.title('Systolic Blood Pressure')
plt.show()

plt.figure(figsize=(5, 4))
plt.boxplot(df['Diastolic'])
plt.title('Diastolic Blood Pressure')
plt.show()

import matplotlib.pyplot as plt
plt.figure(figsize=(5,4))
plt.boxplot(df['Cholesterol'])
plt.title('Cholesterol')
plt.show()

"""HEAT MAP"""

plt.figure(figsize=(20,18))
var1=df.corr(numeric_only=True)
sns.heatmap(var1,annot=True)
plt.title('Heat Map')
plt.show()

"""CATEGORICAL VALUE ENCODING"""

obj1=[]
for i in df:
 if df[i].dtype=='object':
  obj1.append(i)

obj1

"""LABEL ENCODING"""

label_Enc=[]
Onehot_Enc=[]
for i in df:
 if df[i].dtype=='object' and df[i].nunique()>2:
  Onehot_Enc.append(i)
 elif df[i].dtype=='object' and df[i].nunique()<=2:
  label_Enc.append(i)

label_Enc

df['Sex']

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['Sex']=le.fit_transform(df['Sex'])

le.inverse_transform(df['Sex'])

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Diet'] = le.fit_transform(df['Diet'])

le.inverse_transform(df['Sex'])

"""ONE HOT ENCODING"""

from sklearn.preprocessing import OneHotEncoder
onehot=OneHotEncoder(sparse_output=False,drop='first')
result=onehot.fit_transform(df[Onehot_Enc])

onehot.get_feature_names_out()

result=pd.DataFrame(result,columns=onehot.get_feature_names_out())

result

df=df.drop(columns=Onehot_Enc)

df.reset_index(drop=True, inplace=True)

df=df.join(result)

x = df.drop(columns=['Heart Attack Risk'])
y = df['Heart Attack Risk']

from sklearn.preprocessing import LabelEncoder

for col in x.select_dtypes(include='object').columns:
    le = LabelEncoder()
    x[col] = le.fit_transform(x[col])

"""SAMPLING"""

from imblearn.over_sampling import SMOTE

sampler = SMOTE()
x_resample, y_resample = sampler.fit_resample(x, y)

print(y_resample.value_counts())

"""SCALING"""

from sklearn.preprocessing import MinMaxScaler
minmax=MinMaxScaler()
x_scaled=minmax.fit_transform(x_resample)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=0.01)
x = sel.fit_transform(x)

"""DATA SPLITTING"""

# Before SMOTE
x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.2, random_state=42)
model.fit(x_train1, y_train1)
print("Before SMOTE:", model.score(x_test1, y_test1))

# After SMOTE
model.fit(x_train, y_train)
print("After SMOTE:", model.score(x_test, y_test))

"""**Random Forest**

Random Forest is an ensemble method that builds multiple decision trees and combines their outputs. It reduces overfitting and increases accuracy by averaging (for regression) or taking the majority vote (for classification).

Used for: Classification, Regression

Idea: "A forest of trees decides better than one tree."
"""

x_train, x_test, y_train, y_test = train_test_split(x_resample, y_resample, test_size=0.2, random_state=50)
model = RandomForestClassifier()
model.fit(x_train, y_train)
print(model.score(x_test, y_test))

"""**Logistic Regression**

Logistic Regression is used for binary and multiclass classification tasks. Despite the name, it's a classification model that uses a sigmoid function to estimate probabilities and classify inputs based on a threshold.

  Used for: Classification

  Idea: "Estimate the probability of a class using a squished 'S' curve."
"""

model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(x_train, y_train)
print(model.score(x_test, y_test))

"""K NEAREST NEIGHBOR

K-Nearest Neighbors (KNN) is a simple, intuitive, and versatile machine learning algorithm used for both classification and regression tasks. It is based on the idea that similar data points tend toexist near each other in the feature space.
"""

from sklearn.neighbors import KNeighborsClassifier
knn_model=KNeighborsClassifier()
knn_model.fit(x_test,y_test)

y_pred=knn_model.predict(x_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
print(accuracy_score(y_test,y_pred))

"""**Decision Tree**

A Decision Tree is a model that splits the data into branches based on feature values, creating a tree-like structure. It makes decisions by asking a series of yes/no questions based on feature thresholds.

Used for: Classification, Regression

Idea: "Split data recursively based on the most informative questions."
"""

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(x_train, y_train)
print("Decision Tree Accuracy:", model.score(x_test, y_test))

"""**Support Vector Machine (SVM)**

SVM is a powerful classification algorithm that finds the optimal hyperplane that separates classes with the maximum margin. For non-linear problems, it uses kernel functions to map data into higher dimensions.

Used for: Classification, sometimes Regression

Idea: "Find the boundary that best separates classes with the largest margin."
"""

from sklearn.svm import SVC
model = SVC()
model.fit(x_train, y_train)
print("SVM Accuracy:", model.score(x_test, y_test))

"""**Naive Bayes**

Naive Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that the features are conditionally independent (naive assumption). Despite its simplicity, it's effective for many text classification problems.

Used for: Classification

Idea: "Use probability and assume everything is independent."
"""

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(x_train, y_train)
print("Naive Bayes Accuracy:", model.score(x_test, y_test))

"""**Gradient Boosting**

Gradient Boosting is an ensemble method that builds models sequentially, where each new model tries to correct the errors of the previous ones. Popular implementations like XGBoost and LightGBM are highly efficient and accurate.

Used for: Classification, Regression

Idea: "Fix previous mistakes with smarter and smarter models."
"""

from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier()
model.fit(x_train, y_train)
print("Gradient Boosting Accuracy:", model.score(x_test, y_test))

"""In this project, machine learning is used to predict the risk of heart attacks based on a person’s medical history and health features. To find the most accurate model, multiple algorithms were tested including Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, Random Forest, Support Vector Machine (SVM), Naive Bayes, and Gradient Boosting. Initially, the dataset was imbalanced, with fewer positive cases, which negatively impacted model performance. To address this, SMOTE (Synthetic Minority Over-sampling Technique) was applied, which generated synthetic samples for the minority class and significantly improved prediction accuracy. After balancing the data, Random Forest achieved the highest accuracy of 86.88%, compared to just 64.17% before applying SMOTE. This shows that balancing the dataset plays a crucial role in medical predictions. Among all models, Random Forest performed the best due to its ensemble nature and ability to handle complex patterns, making it a strong candidate for heart attack prediction systems."""